{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[maddpg_algorithm]: https://github.com/Brandon-HY-Lin/deep-reinforcement-learning/blob/master/p3_collab-compet/MADDPG/picures/maddpg_algorithm.png \"Algorithm of MADDPG\"\n",
    "\n",
    "[maddpg_digram]: https://github.com/Brandon-HY-Lin/deep-reinforcement-learning/blob/master/p3_collab-compet/MADDPG/picures/maddpg_diagram.png \"Diagram of MADDPG\"\n",
    "\n",
    "\n",
    "# Abstract\n",
    "\n",
    "# Introduction\n",
    "By looking at the highlighted equations in MADDPG algorithm shown below, the major difference between MADDPG and DDPG is the input shape of critic. Note that, x = \\[o1, o2, ..., oN\\]. The input size of critic in MADDPG algorithm is __(state_size+action_size)*num_agents__. On the other hand, the input size of actor in MADDPG algorithm is the same as DDPG, i.e. __state_size__.\n",
    "\n",
    "![Algorithm of MADDPG][maddpg_algorithm]\n",
    "\n",
    "*Algorthm of MADDPG*\n",
    "\n",
    "\n",
    "Furthermore, the diagram shown below also illustrate this subtile difference.\n",
    "![Diagram of MADDPG][maddpg_digram]\n",
    "\n",
    "*Diagram of MADDPG*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
