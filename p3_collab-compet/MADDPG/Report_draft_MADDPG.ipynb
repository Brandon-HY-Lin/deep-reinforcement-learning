{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[maddpg_algorithm]: https://github.com/Brandon-HY-Lin/deep-reinforcement-learning/blob/master/p3_collab-compet/MADDPG/picures/maddpg_algorithm.png \"Algorithm of MADDPG\"\n",
    "\n",
    "[maddpg_digram]: https://github.com/Brandon-HY-Lin/deep-reinforcement-learning/blob/master/p3_collab-compet/MADDPG/picures/maddpg_diagram.png \"Diagram of MADDPG\"\n",
    "\n",
    "[img_maddpg_version_11]: https://github.com/Brandon-HY-Lin/deep-reinforcement-learning/blob/master/p3_collab-compet/MADDPG/picures/MADDPG_version_11.png \"Score of Version 11\"\n",
    "\n",
    "\n",
    "# Abstract\n",
    "This work adopts [MADDPG](https://arxiv.org/abs/1706.02275) to play tennis game which is similar to [Unity's Tennis game](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#tennis) and achieve a score of 2.7. The average score reaches +0.5 at episode 466.\n",
    "\n",
    "# Introduction\n",
    "By looking at the highlighted text in [MADDPG](https://arxiv.org/abs/1706.02275) algorithm shown below, the major difference between MADDPG and DDPG is the input shape of critic. Note that, x = \\[o1, o2, ..., oN\\]. The input size of critic in MADDPG algorithm is __(state_size+action_size)*num_agents__. On the other hand, the input size of actor in MADDPG algorithm is the same as DDPG, i.e. __state_size__.\n",
    "\n",
    "![Algorithm of MADDPG][maddpg_algorithm]\n",
    "\n",
    "*Algorthm of MADDPG*\n",
    "\n",
    "\n",
    "Furthermore, the diagram shown below also illustrate this subtile difference. For N agents, there are N policy-networks, N Q-networks, and only 1 replay buffer.\n",
    "\n",
    "![Diagram of MADDPG][maddpg_digram]\n",
    "\n",
    "*Diagram of MADDPG*\n",
    "\n",
    "The translucent line shows the raw score of every episode. The solid line shows the average score with window=100. \n",
    "\n",
    "\n",
    "# Results\n",
    "The average score reaches +0.5 at episode 466. The highest score is 2.7 at episode 937. The stability of score is still an issue despite adding batch normalization.\n",
    "\n",
    "![Score of MADDPG version 11][img_maddpg_version_11]\n",
    "\n",
    "\n",
    "# Appendix\n",
    "\n",
    "### Hyper-Parameters\n",
    "* state_size : 24\n",
    "* action_size : 2\n",
    "* lr_critic : 1e-3 (learning rate of critic using Adam optimizer)\n",
    "* lr_actor : 1e-3  (learning rate of actor using Adam optimizer)\n",
    "* fc1_units : 256\n",
    "* fc2_units : 128\n",
    "* gamma : 0.95     (discount rate of reward)\n",
    "* tau : 1e-2       (parameter of soft update)\n",
    "* max_norm : 1.0   (gradient clipping)\n",
    "* epsilon_start : 5.0     (starting ratio of exploration)\n",
    "* epsilon_end : 0.0       (ending ratio of exploration)\n",
    "* epsilon_decay : 0.99    (decay rate of exploration)\n",
    "* learn_period : 10       (training period)\n",
    "* learn_sampling_num :10  (number of training in each period)\n",
    "* buffer_size : int(1e6)\n",
    "* batch_size : 256\n",
    "\n",
    "\n",
    "### Design Patterns\n",
    "Because each agent needs the other agent to predict its next actions, the [chain-of-responsibility](https://en.wikipedia.org/wiki/Chain-of-responsibility_pattern) design pattern, i.e. broker chain, is used in this project. The Game() class stores 2 agents and each agent also contains Game() class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "[0 1 2 3]\n",
      "0\n",
      "1\n",
      "[0 1 2 3]\n",
      "0\n",
      "1\n",
      "[0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ActionQuery():\n",
    "    def __init__(self):\n",
    "        self.result = None\n",
    "        \n",
    "\n",
    "class Game():\n",
    "    def __init__(self):\n",
    "        self.players = []\n",
    "        self.index = 0\n",
    "        \n",
    "    def accept(self, player):\n",
    "        self.players.append(player)\n",
    "        \n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.index = 0\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.index < self.__len__():\n",
    "            player = self.players[self.index]\n",
    "            self.index += 1\n",
    "            return player\n",
    "        else:\n",
    "            raise StopIteration\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.players)\n",
    "        \n",
    "        \n",
    "class Player():\n",
    "    def __init__(self, game, version):\n",
    "        self.game = game\n",
    "        self.version = version\n",
    "        \n",
    "        \n",
    "    def forward_all(self):\n",
    "        actionQuery = ActionQuery()\n",
    "        for i_player, player in enumerate(self.game):\n",
    "            print(i_player)\n",
    "            player.query(actionQuery)\n",
    "        \n",
    "        return actionQuery.result\n",
    "    \n",
    "    \n",
    "    def query(self, actionQuery):\n",
    "        if actionQuery.result is None:\n",
    "            actionQuery.result = self.version\n",
    "        else:\n",
    "#             actionQuery.result = np.concatenate(actionQuery.result, self.version)\n",
    "            actionQuery.result = np.concatenate((actionQuery.result, self.version), axis=0)\n",
    "            \n",
    "            \n",
    "game = Game()\n",
    "player_0 = Player(game, np.array([0, 1]))\n",
    "player_1 = Player(game, np.array([2, 3]))\n",
    "\n",
    "game.accept(player_0)\n",
    "game.accept(player_1)\n",
    "\n",
    "print(player_0.forward_all())\n",
    "print(player_0.forward_all())\n",
    "print(player_0.forward_all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
