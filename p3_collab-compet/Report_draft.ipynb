{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[maddpg_algorithm]: https://github.com/Brandon-HY-Lin/deep-reinforcement-learning/blob/master/p3_collab-compet/MADDPG/picures/maddpg_algorithm.png \"Algorithm of MADDPG\"\n",
    "\n",
    "[maddpg_digram]: https://github.com/Brandon-HY-Lin/deep-reinforcement-learning/blob/master/p3_collab-compet/MADDPG/picures/maddpg_diagram.png \"Diagram of MADDPG\"\n",
    "\n",
    "[maddpg_ddpg_comparision]: https://github.com/Brandon-HY-Lin/deep-reinforcement-learning/blob/master/p3_collab-compet/MADDPG/picures/maddpg_v11_ddpg_v6_v7.png \"Score of MADDPG version 11, DDPG version 6, and DDPG version 7\"\n",
    "\n",
    "# Abstract\n",
    "This work adopts [MADDPG](https://arxiv.org/abs/1706.02275) to play tennis game which is similar to [Unity's Tennis game](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#tennis) and achieve a maximal score of 2.7. Its highest average score is +1.2 over 100 episodes. Beside MADDPG, [DDPG](https://arxiv.org/abs/1509.02971) with [prioritized experience replay (PER)](https://arxiv.org/abs/1511.05952) is also implemented and achieve similar score. The highest score is +2.6 and the best average score is +0.8 (over 100 episodes).\n",
    "\n",
    "# Introduction\n",
    "Two-player game where agents control rackets to bounce ball over a net. The agents must bounce ball between one another while not dropping or sending ball out of bounds. The environment contains two agent linked to a single Brain. \n",
    "\n",
    "* Agent Reward Function (independent):\n",
    "\t* +0.1 To agent when hitting ball over net.\n",
    "\t* -0.1 To agent who let ball hit their ground, or hit ball out of bounds.\n",
    "\n",
    "* Brains: One Brain with the following observation/action space.\n",
    "\t* Vector Observation space: 8 variables corresponding to position and velocity of ball and racket.\n",
    "\t* Vector Action space: (Continuous) Size of 2, corresponding to movement toward net or away from net, and jumping.\n",
    "\t* Visual Observations: None.\n",
    "\n",
    "In order to solve this game, the agents must get average score of +0.5 over 100 consecutive episodes. The score of each episode is calculated by taking maximal scores of 2 agents.\n",
    "\n",
    "\n",
    "# MADDPG Algorithm\n",
    "By looking at the highlighted text in [MADDPG](https://arxiv.org/abs/1706.02275) algorithm shown below, the major difference between MADDPG and DDPG is the input shape of critic. Note that, x = \\[o1, o2, ..., oN\\]. The input size of critic in MADDPG algorithm is __(state_size+action_size)*num_agents__. On the other hand, the input size of actor in MADDPG algorithm is the same as DDPG, i.e. __state_size__.\n",
    "\n",
    "![Algorithm of MADDPG][maddpg_algorithm]\n",
    "\n",
    "*Algorthm of MADDPG*\n",
    "\n",
    "\n",
    "Furthermore, the diagram shown below also illustrate this subtile difference. For N agents, there are N policy-networks, N Q-networks, and only 1 replay buffer. Note that, each agent's critic needs to get the other agent's next actions. To solve this problem, the [chain-of-responsibility](https://en.wikipedia.org/wiki/Chain-of-responsibility_pattern) design pattern, i.e. broker chain, is used.\n",
    "\n",
    "![Diagram of MADDPG][maddpg_digram]\n",
    "\n",
    "*Diagram of MADDPG*\n",
    "\n",
    "\n",
    "\n",
    "# Results\n",
    "Three experiments are listed below. Two of them are DDPG, the other is MADDPG. DDPG version 6 and MADDPG version 11 have batch-norm at input of critic and actor netorks. The MADDPG has better result and reaches average score of +0.5 at episode 466. And its highest average score is about +1.2.\n",
    "\n",
    "\n",
    "![Diagram of MADDPG and DDPG comparison][maddpg_ddpg_comparision]\n",
    "\n",
    "*Scores of MADDPG and DDPG*\n",
    "\n",
    "# Conclusion\n",
    "In this work, two algorithms are implemented and both achieve good scores of over +2.5. During experiments, one key factor is noticed: the performances of both algorithms are highly influenced by the number of training steps. The more it is, the faster the score gets higher. Besides this, stability is still a big issue even if PER is applied as shown in DDPG version.\n",
    "\n",
    "\n",
    "# Future Works\n",
    "1. Add PER (Prioritized Experience Replay) to MADDPG. Although shared replay buffer is used by two agents, it worths a try to add replay buffer to one of the agent.\n",
    "2. Adopt [twin delayed DDPG](https://spinningup.openai.com/en/latest/algorithms/td3.html). This variant has 3 tricks, some of them are applied in current project. So, it woundn't too much efforts to do.\n",
    "\n",
    "\n",
    "# Appendix\n",
    "\n",
    "#### Report of DDPG\n",
    "Check this [link](https://github.com/Brandon-HY-Lin/deep-reinforcement-learning/blob/master/p3_collab-compet/DDPG/Report.md) to see detailed report of DDPG.\n",
    "\n",
    "\n",
    "#### Hyper-Parameters\n",
    "\n",
    "* MADDPG\n",
    "    * state_size : 24\n",
    "    * action_size : 2\n",
    "    * lr_critic : 1e-3 (learning rate of critic using Adam optimizer)\n",
    "    * lr_actor : 1e-3  (learning rate of actor using Adam optimizer)\n",
    "    * fc1_units : 256\n",
    "    * fc2_units : 128\n",
    "    * gamma : 0.95     (discount rate of reward)\n",
    "    * tau : 1e-2       (parameter of soft update)\n",
    "    * max_norm : 1.0   (gradient clipping)\n",
    "    * epsilon_start : 5.0     (starting ratio of exploration)\n",
    "    * epsilon_end : 0.0       (ending ratio of exploration)\n",
    "    * epsilon_decay : 0.99    (decay rate of exploration)\n",
    "    * learn_period : 10       (training period)\n",
    "    * learn_sampling_num :10  (number of training in each period)\n",
    "    * buffer_size : int(1e6)\n",
    "    * batch_size : 256\n",
    "\n",
    "\n",
    "* DDPG\n",
    "    * state_size : 24\n",
    "    * action_size : 2\n",
    "    * lr_actor : 1e-3          (learning rate of actor using Adam optimizer)\n",
    "    * lr_critic : 1e-3         (learning rate of critic using Adam optimizer)\n",
    "    * fc1_units : 256\n",
    "    * fc2_units : 128\n",
    "    * buffer_size : int(1e6)\n",
    "    * learn_period : 10         (training period)\n",
    "    * learn_sampling_num : 20   (number of training in each period)\n",
    "    * batch_size : 128\n",
    "    * max_norm : 1.0            (gradient clipping)\n",
    "    * exploration_sigma : 0.2   (parameter of OU noise process)\n",
    "    * epsilon_start : 5.0       (starting ratio of exploration)\n",
    "    * epsilon_end : 0.0         (ending ratio of exploration)\n",
    "    * epsilon_decay : 0.0       (decay rate of exploration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
